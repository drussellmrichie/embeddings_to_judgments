{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/all_vec_dfs.pickle\", \"rb\") as handle:\n",
    "#     all_vec_dfs = pickle.load(handle)\n",
    "\n",
    "with open(\"data/all_vector_dfs.pickle\", \"rb\") as handle:\n",
    "    all_vec_dfs = pickle.load(handle)\n",
    "\n",
    "with open(\"data/dict_of_ys.pickle\", \"rb\") as handle:\n",
    "    dict_of_ys = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sudeep suggested we remove SWOW_RW because it's not derived from text, and the title of our paper refers to semantic representations from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vec_dfs.pop('swow_rw', None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word2vec',\n",
       " 'fasttext',\n",
       " 'glove',\n",
       " 'elmo_decontext',\n",
       " 'elmo_context',\n",
       " 'paragram',\n",
       " 'glove_postspec',\n",
       " 'bert_decontext',\n",
       " 'bert_context']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_types = list(all_vec_dfs.keys())\n",
    "vector_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>capable</th>\n",
       "      <td>-0.021387</td>\n",
       "      <td>0.035702</td>\n",
       "      <td>0.065195</td>\n",
       "      <td>-0.027423</td>\n",
       "      <td>-0.109693</td>\n",
       "      <td>0.072439</td>\n",
       "      <td>0.030873</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>-0.031390</td>\n",
       "      <td>0.041049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.020438</td>\n",
       "      <td>-0.041393</td>\n",
       "      <td>-0.030010</td>\n",
       "      <td>-0.136598</td>\n",
       "      <td>0.100034</td>\n",
       "      <td>-0.080027</td>\n",
       "      <td>-0.039496</td>\n",
       "      <td>-0.028113</td>\n",
       "      <td>0.043291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daring</th>\n",
       "      <td>0.033601</td>\n",
       "      <td>0.023445</td>\n",
       "      <td>0.010584</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>0.065684</td>\n",
       "      <td>-0.134405</td>\n",
       "      <td>0.029235</td>\n",
       "      <td>0.034550</td>\n",
       "      <td>0.043853</td>\n",
       "      <td>0.054673</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041575</td>\n",
       "      <td>0.019743</td>\n",
       "      <td>-0.090363</td>\n",
       "      <td>-0.000320</td>\n",
       "      <td>-0.083149</td>\n",
       "      <td>-0.094919</td>\n",
       "      <td>0.044802</td>\n",
       "      <td>0.024299</td>\n",
       "      <td>-0.022876</td>\n",
       "      <td>0.120737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.065318</td>\n",
       "      <td>0.015824</td>\n",
       "      <td>0.023063</td>\n",
       "      <td>-0.015403</td>\n",
       "      <td>0.061614</td>\n",
       "      <td>0.006776</td>\n",
       "      <td>0.119861</td>\n",
       "      <td>-0.008501</td>\n",
       "      <td>0.140063</td>\n",
       "      <td>0.036531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050167</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.050503</td>\n",
       "      <td>-0.050503</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>-0.018097</td>\n",
       "      <td>-0.021380</td>\n",
       "      <td>0.028450</td>\n",
       "      <td>-0.008796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cheerful</th>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.085966</td>\n",
       "      <td>-0.012719</td>\n",
       "      <td>0.034562</td>\n",
       "      <td>-0.058597</td>\n",
       "      <td>-0.024562</td>\n",
       "      <td>0.101054</td>\n",
       "      <td>0.087370</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.067369</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026141</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>0.010921</td>\n",
       "      <td>-0.075791</td>\n",
       "      <td>-0.033158</td>\n",
       "      <td>0.033685</td>\n",
       "      <td>0.040702</td>\n",
       "      <td>-0.043685</td>\n",
       "      <td>0.117195</td>\n",
       "      <td>0.023158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>committed</th>\n",
       "      <td>-0.064982</td>\n",
       "      <td>0.133554</td>\n",
       "      <td>0.133554</td>\n",
       "      <td>-0.030516</td>\n",
       "      <td>-0.061751</td>\n",
       "      <td>0.046672</td>\n",
       "      <td>0.087241</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.062110</td>\n",
       "      <td>-0.019297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033927</td>\n",
       "      <td>0.084728</td>\n",
       "      <td>-0.073598</td>\n",
       "      <td>-0.048467</td>\n",
       "      <td>-0.097652</td>\n",
       "      <td>-0.000718</td>\n",
       "      <td>-0.001806</td>\n",
       "      <td>0.049544</td>\n",
       "      <td>0.084728</td>\n",
       "      <td>0.015079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5    \\\n",
       "capable   -0.021387  0.035702  0.065195 -0.027423 -0.109693  0.072439   \n",
       "daring     0.033601  0.023445  0.010584  0.008685  0.065684 -0.134405   \n",
       "sad        0.065318  0.015824  0.023063 -0.015403  0.061614  0.006776   \n",
       "cheerful   0.039474  0.085966 -0.012719  0.034562 -0.058597 -0.024562   \n",
       "committed -0.064982  0.133554  0.133554 -0.030516 -0.061751  0.046672   \n",
       "\n",
       "                6         7         8         9    ...       290       291  \\\n",
       "capable    0.030873  0.003234 -0.031390  0.041049  ...  0.002619  0.020438   \n",
       "daring     0.029235  0.034550  0.043853  0.054673  ... -0.041575  0.019743   \n",
       "sad        0.119861 -0.008501  0.140063  0.036531  ... -0.050167  0.011532   \n",
       "cheerful   0.101054  0.087370  0.045264  0.067369  ... -0.026141  0.004057   \n",
       "committed  0.087241  0.000875  0.062110 -0.019297  ... -0.033927  0.084728   \n",
       "\n",
       "                292       293       294       295       296       297  \\\n",
       "capable   -0.041393 -0.030010 -0.136598  0.100034 -0.080027 -0.039496   \n",
       "daring    -0.090363 -0.000320 -0.083149 -0.094919  0.044802  0.024299   \n",
       "sad        0.000164  0.050503 -0.050503  0.011700 -0.018097 -0.021380   \n",
       "cheerful   0.010921 -0.075791 -0.033158  0.033685  0.040702 -0.043685   \n",
       "committed -0.073598 -0.048467 -0.097652 -0.000718 -0.001806  0.049544   \n",
       "\n",
       "                298       299  \n",
       "capable   -0.028113  0.043291  \n",
       "daring    -0.022876  0.120737  \n",
       "sad        0.028450 -0.008796  \n",
       "cheerful   0.117195  0.023158  \n",
       "committed  0.084728  0.015079  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vec_dfs['word2vec'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec (2716, 300)\n",
      "fasttext (2164, 300)\n",
      "glove (2166, 300)\n",
      "elmo_decontext (1400, 1024)\n",
      "elmo_context (1400, 1024)\n",
      "paragram (1059, 300)\n",
      "glove_postspec (982, 300)\n",
      "bert_decontext (1400, 768)\n",
      "bert_context (1400, 768)\n"
     ]
    }
   ],
   "source": [
    "for vec_type, vec_df in all_vec_dfs.items():\n",
    "    print(vec_type, vec_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_ser_per_dom = [ser for ser in dict_of_ys.values()][::2]\n",
    "len(one_ser_per_dom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_domain_order = ['brand', 'product', 'trait', 'food', 'occupation', 'risk', 'people']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word2vec</th>\n",
       "      <th>fasttext</th>\n",
       "      <th>glove</th>\n",
       "      <th>elmo_decontext</th>\n",
       "      <th>elmo_context</th>\n",
       "      <th>paragram</th>\n",
       "      <th>glove_postspec</th>\n",
       "      <th>bert_decontext</th>\n",
       "      <th>bert_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>199</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>136</td>\n",
       "      <td>89</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>200</td>\n",
       "      <td>191</td>\n",
       "      <td>191</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>191</td>\n",
       "      <td>185</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trait</th>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>197</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>152</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occupation</th>\n",
       "      <td>200</td>\n",
       "      <td>178</td>\n",
       "      <td>179</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>179</td>\n",
       "      <td>169</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>risk</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>198</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>197</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word2vec  fasttext  glove  elmo_decontext  elmo_context  paragram  \\\n",
       "brand            199       135    135             199           199       136   \n",
       "product          200       191    191             200           200       191   \n",
       "trait            200       199    199             200           200       199   \n",
       "food             162       162    162             162           162       162   \n",
       "occupation       200       178    179             200           200       179   \n",
       "risk             200       200    200             200           200       200   \n",
       "people           197        17     17             197           197        17   \n",
       "\n",
       "            glove_postspec  bert_decontext  bert_context  \n",
       "brand                   89             199           199  \n",
       "product                185             200           200  \n",
       "trait                  197             200           200  \n",
       "food                   152             162           162  \n",
       "occupation             169             200           200  \n",
       "risk                   198             200           200  \n",
       "people                  17             197           197  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_count_df = pd.DataFrame(index=old_domain_order, columns=all_vec_dfs.keys())\n",
    "\n",
    "for domain, y_series in zip(old_domain_order, one_ser_per_dom):\n",
    "    for vec_type, vec_df in all_vec_dfs.items():\n",
    "        if vec_type in ['paragram','glove_postspec']:\n",
    "            needed_items = pd.Series([x.lower().replace(' ','_') for x in y_series.index])\n",
    "        else:\n",
    "            needed_items = pd.Series([x.replace(' ','_') for x in y_series.index])\n",
    "            \n",
    "#         if vec_type == 'elmo_context':\n",
    "        if '_context' in vec_type:\n",
    "            vec_df = vec_df.loc[domain]\n",
    "            count_of_items_with_vectors = sum(needed_items.isin(vec_df.index))\n",
    "        else:\n",
    "            count_of_items_with_vectors = sum(needed_items.isin(vec_df.index))\n",
    "            \n",
    "        item_count_df.loc[domain, vec_type] = count_of_items_with_vectors\n",
    "\n",
    "item_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec doesn't have 200 in every domain because we've already dropped some items according to pre-reg criteria.\n",
    "\n",
    "So we should be able to make at least decent predictions for all domains and embeddings, except perhaps brands and especially people.\n",
    "\n",
    "For brands, we can retain the items in the glove_postspec vocabularies.\n",
    "\n",
    "For people, since just doing word2vec and the two elmo models, don't throw any out.\n",
    "\n",
    "For the rest, retain the number of items in the swow_rw vocabularies.\n",
    "\n",
    "May want to reduce all the spaces so that p == n (number of features == number of items)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_count_df.to_excel('data/item_counts_per_space.xlsx')\n",
    "item_count_df.to_excel('data/item_counts_per_space_no_swow_rw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restrict items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_domain_order = ['trait', 'risk', 'people', 'food', 'occupation', 'brand', 'product']\n",
    "\n",
    "dims = [('masculine', 'feminine'),\n",
    "       ('dread-inducing', 'unknowable'),\n",
    "       ('warm', 'competent'),\n",
    "       ('tasty', 'nutritious'),\n",
    "       ('significance', 'autonomy'),\n",
    "       ('sincere', 'exciting'),\n",
    "       ('hedonic', 'utilitarian')]\n",
    "\n",
    "# restricting_vec_types = ['swow_rw', 'swow_rw', 'word2vec', 'swow_rw', 'swow_rw', 'glove_postspec', 'swow_rw']\n",
    "restricting_vec_types = ['glove_postspec', 'glove_postspec', 'word2vec', 'glove_postspec', 'glove_postspec', 'glove_postspec', 'glove_postspec'] # without swow_rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "restricting_items_dict = dict()\n",
    "\n",
    "for domain, dim_pair, vec_type in zip(new_domain_order, dims, restricting_vec_types):\n",
    "    dim = dim_pair[0] # only need to do one dim\n",
    "    y_series = dict_of_ys[dim]\n",
    "    vec_df = all_vec_dfs[vec_type]\n",
    "    \n",
    "    if vec_type in ['paragram', 'glove_postspec']:\n",
    "        curr_domain_items = pd.Series([x.lower().replace(' ','_') for x in y_series.index])\n",
    "    else:\n",
    "        curr_domain_items = pd.Series([x.replace(' ','_') for x in y_series.index])\n",
    "        \n",
    "    curr_domain_items_with_vectors = curr_domain_items[curr_domain_items.isin(vec_df.index)].values\n",
    "    restricting_items_dict[domain] = curr_domain_items_with_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trait</th>\n",
       "      <th>risk</th>\n",
       "      <th>people</th>\n",
       "      <th>food</th>\n",
       "      <th>occupation</th>\n",
       "      <th>brand</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capable</td>\n",
       "      <td>car</td>\n",
       "      <td>Frank_Sinatra</td>\n",
       "      <td>stuffing</td>\n",
       "      <td>mercenary</td>\n",
       "      <td>bose</td>\n",
       "      <td>hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daring</td>\n",
       "      <td>weapons</td>\n",
       "      <td>Confucius</td>\n",
       "      <td>peas</td>\n",
       "      <td>janitor</td>\n",
       "      <td>nbc</td>\n",
       "      <td>nightgown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sad</td>\n",
       "      <td>plane</td>\n",
       "      <td>Eric_Clapton</td>\n",
       "      <td>tripe</td>\n",
       "      <td>babysitter</td>\n",
       "      <td>pfizer</td>\n",
       "      <td>skirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cheerful</td>\n",
       "      <td>scammers</td>\n",
       "      <td>Judy_Garland</td>\n",
       "      <td>semolina</td>\n",
       "      <td>psychologist</td>\n",
       "      <td>firestone</td>\n",
       "      <td>ink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>committed</td>\n",
       "      <td>tobacco</td>\n",
       "      <td>Julia_Child</td>\n",
       "      <td>lemons</td>\n",
       "      <td>groundskeeper</td>\n",
       "      <td>facebook</td>\n",
       "      <td>ski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grateful</td>\n",
       "      <td>lightning</td>\n",
       "      <td>Roald_Dahl</td>\n",
       "      <td>avocado</td>\n",
       "      <td>dancer</td>\n",
       "      <td>palmolive</td>\n",
       "      <td>photography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>depressed</td>\n",
       "      <td>gun</td>\n",
       "      <td>Lyndon_B._Johnson</td>\n",
       "      <td>oranges</td>\n",
       "      <td>programmer</td>\n",
       "      <td>samsung</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>arrogant</td>\n",
       "      <td>riot</td>\n",
       "      <td>Frida_Kahlo</td>\n",
       "      <td>goulash</td>\n",
       "      <td>baker</td>\n",
       "      <td>toyota</td>\n",
       "      <td>paperclip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dedicated</td>\n",
       "      <td>pollution</td>\n",
       "      <td>Abraham_Lincoln</td>\n",
       "      <td>flapjacks</td>\n",
       "      <td>professor</td>\n",
       "      <td>pontiac</td>\n",
       "      <td>rowboat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dignified</td>\n",
       "      <td>gas</td>\n",
       "      <td>Marie_Antoinette</td>\n",
       "      <td>bacon</td>\n",
       "      <td>priest</td>\n",
       "      <td>kotex</td>\n",
       "      <td>tylenol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trait       risk             people       food     occupation  \\\n",
       "0    capable        car      Frank_Sinatra   stuffing      mercenary   \n",
       "1     daring    weapons          Confucius       peas        janitor   \n",
       "2        sad      plane       Eric_Clapton      tripe     babysitter   \n",
       "3   cheerful   scammers       Judy_Garland   semolina   psychologist   \n",
       "4  committed    tobacco        Julia_Child     lemons  groundskeeper   \n",
       "5   grateful  lightning         Roald_Dahl    avocado         dancer   \n",
       "6  depressed        gun  Lyndon_B._Johnson    oranges     programmer   \n",
       "7   arrogant       riot        Frida_Kahlo    goulash          baker   \n",
       "8  dedicated  pollution    Abraham_Lincoln  flapjacks      professor   \n",
       "9  dignified        gas   Marie_Antoinette      bacon         priest   \n",
       "\n",
       "       brand      product  \n",
       "0       bose          hat  \n",
       "1        nbc    nightgown  \n",
       "2     pfizer        skirt  \n",
       "3  firestone          ink  \n",
       "4   facebook          ski  \n",
       "5  palmolive  photography  \n",
       "6    samsung        apple  \n",
       "7     toyota    paperclip  \n",
       "8    pontiac      rowboat  \n",
       "9      kotex      tylenol  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restricting_items_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in restricting_items_dict.items() ]))\n",
    "restricting_items_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bose', 'nbc', 'pfizer', 'firestone', 'facebook', 'palmolive', 'samsung', 'toyota', 'pontiac', 'kotex', 'hollister', 'aol', 'nabisco', 'gillette', 'nestle', 'volvo', 'honda', 'chanel', 'espn', 'netflix', 'downy', 'meijer', 'nickelodeon', 'ajax', 'huggies', 'fresca', 'charmin', 'fanta', 'cheerios', 'jergens', 'mazda', 'volkswagen', 'maytag', 'reebok', 'google', 'subaru', 'walmart', 'merck', 'oshkosh', 'heinz', 'jello', 'hyundai', 'cheetos', 'ferrari', 'pbs', 'costco', 'ihop', 'lexus', 'advil', 'clinique', 'pampers', 'sony', 'dell', 'nike', 'lego', 'microsoft', 'disney', 'audi', 'cbs', 'prada', 'nba', 'rca', 'chevrolet', 'clorox', 'kmart', 'sephora', 'bbc', 'yahoo', 'gerber', 'purina', 'gatorade', 'nascar', 'msn', 'armani', 'wii', 'snickers', 'itunes', 'hp', 'lipton', 'iphone', 'suzuki', 'visa', 'pepsi', 'sears', 'maybelline', 'tampax', 'smirnoff', 'ipod', 'hbo', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "print(list(restricting_items_df['brand']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trait         197\n",
       "risk          198\n",
       "people        197\n",
       "food          152\n",
       "occupation    169\n",
       "brand          89\n",
       "product       185\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restricting_items_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "warnings.simplefilter(\"once\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brand', 'product', 'trait', 'food', 'occupation', 'risk', 'people']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_domain_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trait', 'risk', 'people', 'food', 'occupation', 'brand', 'product']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_domain_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('masculine', 'feminine'),\n",
       " ('dread-inducing', 'unknowable'),\n",
       " ('warm', 'competent'),\n",
       " ('tasty', 'nutritious'),\n",
       " ('significance', 'autonomy'),\n",
       " ('sincere', 'exciting'),\n",
       " ('hedonic', 'utilitarian')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0213866 ,  0.0357019 ,  0.06519471, ..., -0.0394963 ,\n",
       "        -0.0281131 ,  0.0432907 ],\n",
       "       [ 0.0336013 ,  0.023445  ,  0.0105835 , ...,  0.0242993 ,\n",
       "        -0.0228755 ,  0.1207369 ],\n",
       "       [ 0.0653178 ,  0.0158244 ,  0.0230632 , ..., -0.0213798 ,\n",
       "         0.0284503 , -0.008796  ],\n",
       "       ...,\n",
       "       [ 0.0357937 ,  0.0180345 ,  0.0202372 , ...,  0.0564439 ,\n",
       "        -0.14647891, -0.0007185 ],\n",
       "       [-0.063179  , -0.0072393 , -0.0418423 , ..., -0.0220295 ,\n",
       "         0.0689982 , -0.003966  ],\n",
       "       [-0.0638335 , -0.0517655 ,  0.0330283 , ...,  0.0209603 ,\n",
       "         0.0527182 ,  0.0276294 ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(all_vec_dfs['word2vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['masculine', 'feminine', 'dread-inducing', 'unknowable', 'warm', 'competent', 'tasty', 'nutritious', 'significance', 'autonomy', 'sincere', 'exciting', 'hedonic', 'utilitarian']\n"
     ]
    }
   ],
   "source": [
    "flattened_dims = [dim for x in dims for dim in x]\n",
    "print(flattened_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trait</th>\n",
       "      <th>risk</th>\n",
       "      <th>people</th>\n",
       "      <th>food</th>\n",
       "      <th>occupation</th>\n",
       "      <th>brand</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capable</td>\n",
       "      <td>car</td>\n",
       "      <td>Frank_Sinatra</td>\n",
       "      <td>stuffing</td>\n",
       "      <td>mercenary</td>\n",
       "      <td>bose</td>\n",
       "      <td>hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daring</td>\n",
       "      <td>weapons</td>\n",
       "      <td>Confucius</td>\n",
       "      <td>peas</td>\n",
       "      <td>janitor</td>\n",
       "      <td>nbc</td>\n",
       "      <td>nightgown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sad</td>\n",
       "      <td>plane</td>\n",
       "      <td>Eric_Clapton</td>\n",
       "      <td>tripe</td>\n",
       "      <td>babysitter</td>\n",
       "      <td>pfizer</td>\n",
       "      <td>skirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cheerful</td>\n",
       "      <td>scammers</td>\n",
       "      <td>Judy_Garland</td>\n",
       "      <td>semolina</td>\n",
       "      <td>psychologist</td>\n",
       "      <td>firestone</td>\n",
       "      <td>ink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>committed</td>\n",
       "      <td>tobacco</td>\n",
       "      <td>Julia_Child</td>\n",
       "      <td>lemons</td>\n",
       "      <td>groundskeeper</td>\n",
       "      <td>facebook</td>\n",
       "      <td>ski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grateful</td>\n",
       "      <td>lightning</td>\n",
       "      <td>Roald_Dahl</td>\n",
       "      <td>avocado</td>\n",
       "      <td>dancer</td>\n",
       "      <td>palmolive</td>\n",
       "      <td>photography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>depressed</td>\n",
       "      <td>gun</td>\n",
       "      <td>Lyndon_B._Johnson</td>\n",
       "      <td>oranges</td>\n",
       "      <td>programmer</td>\n",
       "      <td>samsung</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>arrogant</td>\n",
       "      <td>riot</td>\n",
       "      <td>Frida_Kahlo</td>\n",
       "      <td>goulash</td>\n",
       "      <td>baker</td>\n",
       "      <td>toyota</td>\n",
       "      <td>paperclip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dedicated</td>\n",
       "      <td>pollution</td>\n",
       "      <td>Abraham_Lincoln</td>\n",
       "      <td>flapjacks</td>\n",
       "      <td>professor</td>\n",
       "      <td>pontiac</td>\n",
       "      <td>rowboat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dignified</td>\n",
       "      <td>gas</td>\n",
       "      <td>Marie_Antoinette</td>\n",
       "      <td>bacon</td>\n",
       "      <td>priest</td>\n",
       "      <td>kotex</td>\n",
       "      <td>tylenol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trait       risk             people       food     occupation  \\\n",
       "0    capable        car      Frank_Sinatra   stuffing      mercenary   \n",
       "1     daring    weapons          Confucius       peas        janitor   \n",
       "2        sad      plane       Eric_Clapton      tripe     babysitter   \n",
       "3   cheerful   scammers       Judy_Garland   semolina   psychologist   \n",
       "4  committed    tobacco        Julia_Child     lemons  groundskeeper   \n",
       "5   grateful  lightning         Roald_Dahl    avocado         dancer   \n",
       "6  depressed        gun  Lyndon_B._Johnson    oranges     programmer   \n",
       "7   arrogant       riot        Frida_Kahlo    goulash          baker   \n",
       "8  dedicated  pollution    Abraham_Lincoln  flapjacks      professor   \n",
       "9  dignified        gas   Marie_Antoinette      bacon         priest   \n",
       "\n",
       "       brand      product  \n",
       "0       bose          hat  \n",
       "1        nbc    nightgown  \n",
       "2     pfizer        skirt  \n",
       "3  firestone          ink  \n",
       "4   facebook          ski  \n",
       "5  palmolive  photography  \n",
       "6    samsung        apple  \n",
       "7     toyota    paperclip  \n",
       "8    pontiac      rowboat  \n",
       "9      kotex      tylenol  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restricting_items_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_params = [{'alpha':10**x} for x in range(-2,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_train_splits = 25 # TO TEST THIS CODE MORE QUICKLY, SET THIS VALUE CLOSER TO 5 OR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(y_true, y_pred):\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def many_test_train_splits(parameters):\n",
    "    rsquareds = []\n",
    "    rmses = []\n",
    "    # copy vecs_and_judgment because each process of this function will modify it,\n",
    "    # and changing the original might lead to unwanted behavior\n",
    "    # I think the behavior can be replaced by the randomization of train_test_split, but I think I \n",
    "    # was getting weird behavior with that....\n",
    "    vecs_and_judgment_temp = vecs_and_judgment.copy(deep=True)\n",
    "#     vecs_and_judgment_temp.dropna(inplace=True) # this will only drop smartphone for glove_postspec!\n",
    "    for _ in range(n_test_train_splits):\n",
    "        vecs_and_judgment_temp = vecs_and_judgment_temp.sample(frac=1)\n",
    "        X = vecs_and_judgment_temp.iloc[:,:-1]\n",
    "        y = vecs_and_judgment_temp['judgment']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "        regression = Ridge(**parameters)\n",
    "        regression.fit(X=X_train, y=y_train)\n",
    "        y_pred = regression.predict(X=X_test)\n",
    "        rsquared = r2_score(y_test, y_pred)\n",
    "        rmse     = rmse_score(y_test, y_pred)\n",
    "        \n",
    "        rsquareds.append(rsquared)\n",
    "        rmses.append(rmse)\n",
    "    mean_rsquared = np.mean(rsquareds)\n",
    "    mean_rmses = np.mean(rmses)\n",
    "    return mean_rsquared, mean_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_vec_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_vec_dfs = {'bert_decontext': all_vec_dfs['bert_decontext'],\n",
    "#                'bert_context':   all_vec_dfs['bert_context'],\n",
    "#                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec_type, vec_df in all_vec_dfs.items():\n",
    "    vec_df.drop_duplicates(inplace=True) # I really should do this when we *get* the vectors....\n",
    "    print(vec_type)\n",
    "    model_results = []\n",
    "#     if vec_type != 'glove_postspec':\n",
    "#         continue\n",
    "#     if vec_type not in ['elmo_decontext', 'elmo_context']:\n",
    "#         continue\n",
    "    for domain, dim_pair in zip(new_domain_order, dims):\n",
    "        print('\\t', domain)\n",
    "        if ((vec_type =='swow_rw' and domain in ['brand', 'people']) or\n",
    "           (vec_type not in ['word2vec','elmo_context','elmo_decontext','bert_context','bert_decontext'] and domain == 'people')):\n",
    "            # need to just return some place-holder for the eventual dataframe \n",
    "            # since there's not enough data to make it worth building models in these cases\n",
    "            not_enough_place_holder = [('not_enough_vectors', 'not_enough_vectors')] * len(list_of_params)\n",
    "            model_results.append(not_enough_place_holder)\n",
    "            model_results.append(not_enough_place_holder)  # must put in one place holder for each dimension in this domain\n",
    "            continue\n",
    "        \n",
    "#         if vec_type == 'elmo_context':\n",
    "        if '_context' in vec_type:\n",
    "            modifiable_vec_df = vec_df.loc[domain].copy()\n",
    "        else:\n",
    "            modifiable_vec_df = vec_df.copy()\n",
    "\n",
    "        curr_restricted_items    = restricting_items_df[domain].dropna().apply(str.lower).values\n",
    "        modifiable_vec_df.index  = modifiable_vec_df.index.map(str.lower)\n",
    "        restricted_vec_df        = modifiable_vec_df.loc[curr_restricted_items]\n",
    "        restricted_vec_df.dropna(inplace=True)\n",
    "        restricted_vec_df = pd.DataFrame(normalize(restricted_vec_df), index=restricted_vec_df.index, columns=restricted_vec_df.columns)\n",
    "        \n",
    "        for dim in dim_pair:\n",
    "            ys = dict_of_ys[dim]\n",
    "            ys.index = ys.index.map(lambda x: x.lower().replace(' ','_'))\n",
    "            ys = ys.to_frame()\n",
    "            vecs_and_judgment = pd.merge(left=restricted_vec_df, right=ys, left_index=True, right_index=True)\n",
    "            with Pool() as p:\n",
    "                scores_by_hyperparam = [mean_scores for mean_scores in p.map(many_test_train_splits, list_of_params)]\n",
    "            model_results.append(scores_by_hyperparam)\n",
    "    model_results = pd.DataFrame(data=model_results, index=flattened_dims, columns=list_of_params).T\n",
    "    rsquared_df = model_results.applymap(lambda x: x[0])\n",
    "    rmse_df     = model_results.applymap(lambda x: x[1])\n",
    "\n",
    "    rsquared_df.to_csv(f'results/preregistered_models_diff_embeddings/rsquared/{vec_type}_all_judgments.csv', float_format='%.2f')\n",
    "    rmse_df.to_csv(    f'results/preregistered_models_diff_embeddings/rmse/{vec_type}_all_judgments.csv',     float_format='%.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-one-out predicted vs actual correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_vec_dfs = {'fasttext': all_vec_dfs['fasttext']\n",
    "#                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip people and brands now\n",
    "\n",
    "new_domain_order = ['trait', 'risk', 'food', 'occupation', 'product']\n",
    "\n",
    "dims = [('masculine', 'feminine'),\n",
    "       ('dread-inducing', 'unknowable'),\n",
    "       ('tasty', 'nutritious'),\n",
    "       ('significance', 'autonomy'),\n",
    "       ('hedonic', 'utilitarian')]\n",
    "\n",
    "flattened_dims = [dim for x in dims for dim in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec\n",
      "fasttext\n",
      "glove\n",
      "elmo_decontext\n",
      "elmo_context\n",
      "paragram\n",
      "glove_postspec\n",
      "bert_decontext\n",
      "bert_context\n"
     ]
    }
   ],
   "source": [
    "all_vec_type_results = []\n",
    "\n",
    "for vec_type, vec_df in all_vec_dfs.items():\n",
    "    prereg_cv_results = pd.read_csv(f'results/preregistered_models_diff_embeddings/rsquared/{vec_type}_all_judgments.csv', index_col=0)\n",
    "    best_hyperparam = prereg_cv_results.mean(axis=1).idxmax()\n",
    "    \n",
    "    vec_df.drop_duplicates(inplace=True) # I really should do this when we *get* the vectors....\n",
    "    print(vec_type)\n",
    "    curr_vec_type_results = []\n",
    "    for domain, dim_pair in zip(new_domain_order, dims):\n",
    "        \n",
    "        if '_context' in vec_type:\n",
    "            modifiable_vec_df = vec_df.loc[domain].copy()\n",
    "        else:\n",
    "            modifiable_vec_df = vec_df.copy()\n",
    "                  \n",
    "        curr_restricted_items    = restricting_items_df[domain].dropna().apply(str.lower).values\n",
    "        modifiable_vec_df.index  = modifiable_vec_df.index.map(str.lower)\n",
    "        restricted_vec_df        = modifiable_vec_df.loc[curr_restricted_items]\n",
    "        restricted_vec_df.dropna(inplace=True)\n",
    "        restricted_vec_df = pd.DataFrame(normalize(restricted_vec_df), index=restricted_vec_df.index, columns=restricted_vec_df.columns)\n",
    "        \n",
    "        for dim in dim_pair:\n",
    "            ys = dict_of_ys[dim]\n",
    "            ys.index = ys.index.map(lambda x: x.lower().replace(' ','_'))\n",
    "            ys = ys.to_frame()\n",
    "            vecs_and_judgment = pd.merge(left=restricted_vec_df, right=ys, left_index=True, right_index=True)\n",
    "\n",
    "            ridge = Ridge(**literal_eval(best_hyperparam))\n",
    "            loo = LeaveOneOut()\n",
    "            \n",
    "            X = vecs_and_judgment.drop('judgment', axis='columns').values\n",
    "            y = vecs_and_judgment['judgment']\n",
    "            \n",
    "            y_preds = np.zeros(shape=len(y))\n",
    "            for train_index, test_index in loo.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "                ridge.fit(X=X_train, y=y_train)\n",
    "                \n",
    "                y_pred = ridge.predict(X=X_test)[0]\n",
    "                y_preds[test_index] = y_pred\n",
    "            r, _ = pearsonr(y_preds, y)                \n",
    "                \n",
    "            curr_vec_type_results.append(r)\n",
    "    all_vec_type_results.append(curr_vec_type_results)\n",
    "\n",
    "all_vec_type_results = pd.DataFrame(all_vec_type_results, columns=flattened_dims, index=all_vec_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masculine</th>\n",
       "      <th>feminine</th>\n",
       "      <th>dread-inducing</th>\n",
       "      <th>unknowable</th>\n",
       "      <th>tasty</th>\n",
       "      <th>nutritious</th>\n",
       "      <th>significance</th>\n",
       "      <th>autonomy</th>\n",
       "      <th>hedonic</th>\n",
       "      <th>utilitarian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word2vec</th>\n",
       "      <td>0.744364</td>\n",
       "      <td>0.803668</td>\n",
       "      <td>0.886180</td>\n",
       "      <td>0.869653</td>\n",
       "      <td>0.670392</td>\n",
       "      <td>0.843523</td>\n",
       "      <td>0.830329</td>\n",
       "      <td>0.816120</td>\n",
       "      <td>0.836901</td>\n",
       "      <td>0.777157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext</th>\n",
       "      <td>0.761170</td>\n",
       "      <td>0.815241</td>\n",
       "      <td>0.875286</td>\n",
       "      <td>0.835785</td>\n",
       "      <td>0.716770</td>\n",
       "      <td>0.865361</td>\n",
       "      <td>0.793513</td>\n",
       "      <td>0.837047</td>\n",
       "      <td>0.809798</td>\n",
       "      <td>0.755734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove</th>\n",
       "      <td>0.803073</td>\n",
       "      <td>0.851510</td>\n",
       "      <td>0.882312</td>\n",
       "      <td>0.853384</td>\n",
       "      <td>0.706478</td>\n",
       "      <td>0.878480</td>\n",
       "      <td>0.832890</td>\n",
       "      <td>0.805618</td>\n",
       "      <td>0.832507</td>\n",
       "      <td>0.784699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elmo_decontext</th>\n",
       "      <td>0.724660</td>\n",
       "      <td>0.756377</td>\n",
       "      <td>0.900160</td>\n",
       "      <td>0.863206</td>\n",
       "      <td>0.612310</td>\n",
       "      <td>0.741526</td>\n",
       "      <td>0.825330</td>\n",
       "      <td>0.787053</td>\n",
       "      <td>0.788508</td>\n",
       "      <td>0.761295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elmo_context</th>\n",
       "      <td>0.688046</td>\n",
       "      <td>0.750063</td>\n",
       "      <td>0.878969</td>\n",
       "      <td>0.851258</td>\n",
       "      <td>0.455575</td>\n",
       "      <td>0.665412</td>\n",
       "      <td>0.831495</td>\n",
       "      <td>0.779633</td>\n",
       "      <td>0.738257</td>\n",
       "      <td>0.658475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paragram</th>\n",
       "      <td>0.789499</td>\n",
       "      <td>0.838304</td>\n",
       "      <td>0.830493</td>\n",
       "      <td>0.809434</td>\n",
       "      <td>0.663651</td>\n",
       "      <td>0.855733</td>\n",
       "      <td>0.751094</td>\n",
       "      <td>0.668672</td>\n",
       "      <td>0.763002</td>\n",
       "      <td>0.746232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove_postspec</th>\n",
       "      <td>0.836223</td>\n",
       "      <td>0.882729</td>\n",
       "      <td>0.765378</td>\n",
       "      <td>0.757595</td>\n",
       "      <td>0.499151</td>\n",
       "      <td>0.824946</td>\n",
       "      <td>0.759834</td>\n",
       "      <td>0.712590</td>\n",
       "      <td>0.650815</td>\n",
       "      <td>0.650016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_decontext</th>\n",
       "      <td>0.706098</td>\n",
       "      <td>0.719789</td>\n",
       "      <td>0.816435</td>\n",
       "      <td>0.754342</td>\n",
       "      <td>0.464481</td>\n",
       "      <td>0.525528</td>\n",
       "      <td>0.708295</td>\n",
       "      <td>0.717110</td>\n",
       "      <td>0.674476</td>\n",
       "      <td>0.663566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_context</th>\n",
       "      <td>0.723288</td>\n",
       "      <td>0.719834</td>\n",
       "      <td>0.799203</td>\n",
       "      <td>0.720404</td>\n",
       "      <td>0.446455</td>\n",
       "      <td>0.508184</td>\n",
       "      <td>0.648374</td>\n",
       "      <td>0.730803</td>\n",
       "      <td>0.633624</td>\n",
       "      <td>0.618687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                masculine  feminine  dread-inducing  unknowable     tasty  \\\n",
       "word2vec         0.744364  0.803668        0.886180    0.869653  0.670392   \n",
       "fasttext         0.761170  0.815241        0.875286    0.835785  0.716770   \n",
       "glove            0.803073  0.851510        0.882312    0.853384  0.706478   \n",
       "elmo_decontext   0.724660  0.756377        0.900160    0.863206  0.612310   \n",
       "elmo_context     0.688046  0.750063        0.878969    0.851258  0.455575   \n",
       "paragram         0.789499  0.838304        0.830493    0.809434  0.663651   \n",
       "glove_postspec   0.836223  0.882729        0.765378    0.757595  0.499151   \n",
       "bert_decontext   0.706098  0.719789        0.816435    0.754342  0.464481   \n",
       "bert_context     0.723288  0.719834        0.799203    0.720404  0.446455   \n",
       "\n",
       "                nutritious  significance  autonomy   hedonic  utilitarian  \n",
       "word2vec          0.843523      0.830329  0.816120  0.836901     0.777157  \n",
       "fasttext          0.865361      0.793513  0.837047  0.809798     0.755734  \n",
       "glove             0.878480      0.832890  0.805618  0.832507     0.784699  \n",
       "elmo_decontext    0.741526      0.825330  0.787053  0.788508     0.761295  \n",
       "elmo_context      0.665412      0.831495  0.779633  0.738257     0.658475  \n",
       "paragram          0.855733      0.751094  0.668672  0.763002     0.746232  \n",
       "glove_postspec    0.824946      0.759834  0.712590  0.650815     0.650016  \n",
       "bert_decontext    0.525528      0.708295  0.717110  0.674476     0.663566  \n",
       "bert_context      0.508184      0.648374  0.730803  0.633624     0.618687  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vec_type_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_vec_type_results.to_csv('results/preregistered_models_diff_embeddings/out-of-sample_pred-v-actual_corrs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vec_type_results.to_csv('results/preregistered_models_diff_embeddings/out-of-sample_pred-v-actual_corrs_no_swow_rw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
